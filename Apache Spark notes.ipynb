{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bootstrap donated by Juan Carlos :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "    $(\"#homeButton\").remove()\n",
       "    $('body').append('<a href=\"#'+$(\"h1,h2,h3:eq(0)\").attr(\"id\")+'\" style=\"position: fixed; bottom: 10px; right: 10px;\" type=\"button\" class=\"btn btn-success btn-circle btn-lg\"><i class=\"glyphicon glyphicon-link\" id=\"homeButton\"></i></a>');\n",
       "    $(\"#MainMenu\").remove()\n",
       "    var menu = \n",
       "      '<div id=\"MainMenu\" style=\"position: fixed; top: 120px; right: 10px; z-index:6; max-height: 500px;\">'+\n",
       "        '<div class=\"list-group panel\" >'+\n",
       "          '<a href=\"#collapseMenu\" class=\"list-group-item list-group-item-success\" data-toggle=\"collapse\" data-parent=\"#MainMenu\">Indíce<i class=\"fa fa-caret-down\"></i></a>'+\n",
       "          '<div class=\"collapse\" id=\"collapseMenu\" style=\"overflow-y: overlay; max-height: 500px;\">'+\n",
       "          '</div>'+\n",
       "        '</div>'+\n",
       "      '</div>'\n",
       "   \n",
       "    var parent = $(menu)\n",
       "    var arrayIds = []\n",
       "    $(\"h1,h2,h3\").attr(\"id\",function(id,Value){\n",
       "        if(Value != \"\"){\n",
       "            var content = (Value.replace(new RegExp('-', 'g'), ' '));\n",
       "            content = \"&nbsp;\".repeat(parseInt($(this).prop(\"tagName\")[1])*2-1) + content;\n",
       "            $(parent).find(\"#collapseMenu\").append('<a href=\"#'+Value+'\" style=\"position:relative;\" class=\"list-group-item\" data-parent=\"#SubMenu1\">'+content+'</a>');\n",
       "        }\n",
       "    })\n",
       "$('body').append(parent)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "    $(\"#homeButton\").remove()\n",
    "    $('body').append('<a href=\"#'+$(\"h1,h2,h3:eq(0)\").attr(\"id\")+'\" style=\"position: fixed; bottom: 10px; right: 10px;\" type=\"button\" class=\"btn btn-success btn-circle btn-lg\"><i class=\"glyphicon glyphicon-link\" id=\"homeButton\"></i></a>');\n",
    "    $(\"#MainMenu\").remove()\n",
    "    var menu = \n",
    "      '<div id=\"MainMenu\" style=\"position: fixed; top: 120px; right: 10px; z-index:6; max-height: 500px;\">'+\n",
    "        '<div class=\"list-group panel\" >'+\n",
    "          '<a href=\"#collapseMenu\" class=\"list-group-item list-group-item-success\" data-toggle=\"collapse\" data-parent=\"#MainMenu\">Indíce<i class=\"fa fa-caret-down\"></i></a>'+\n",
    "          '<div class=\"collapse\" id=\"collapseMenu\" style=\"overflow-y: overlay; max-height: 500px;\">'+\n",
    "          '</div>'+\n",
    "        '</div>'+\n",
    "      '</div>'\n",
    "   \n",
    "    var parent = $(menu)\n",
    "    var arrayIds = []\n",
    "    $(\"h1,h2,h3\").attr(\"id\",function(id,Value){\n",
    "        if(Value != \"\"){\n",
    "            var content = (Value.replace(new RegExp('-', 'g'), ' '));\n",
    "            content = \"&nbsp;\".repeat(parseInt($(this).prop(\"tagName\")[1])*2-1) + content;\n",
    "            $(parent).find(\"#collapseMenu\").append('<a href=\"#'+Value+'\" style=\"position:relative;\" class=\"list-group-item\" data-parent=\"#SubMenu1\">'+content+'</a>');\n",
    "        }\n",
    "    })\n",
    "$('body').append(parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Notes\n",
    "\n",
    "In order to learn Spark, as stated in https://github.com/jadianes/spark-py-notebooks by Jose A. Dianes, we are going to use 10% of the results of the KDD Cup 1999 Data Set. The reference for those (and these) spark notes is Learning Spark by Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia. this version is in Python 3 :).\n",
    "\n",
    "\n",
    "## KDD Cup 1999\n",
    "\n",
    "The task for the classifier learning contest organized in conjunction with the KDD'99 conference was to learn a predictive model (i.e. a classifier) capable of distinguishing between legitimate and illegitimate connections in a computer network.\n",
    "\n",
    "The data looks like this:\n",
    "\n",
    "|   |     |      |    |     |      |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |      |      |      |      |      |      |      |    |    |      |      |      |      |      |      |      |      |         | \n",
    "|---|-----|------|----|-----|------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|------|------|------|------|------|------|------|----|----|------|------|------|------|------|------|------|------|---------| \n",
    "| 0 | tcp | http | SF | 181 | 5450 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8 | 8 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 | 0.00 | 0.00 | 9  | 9  | 1.00 | 0.00 | 0.11 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | normal. | \n",
    "| 0 | tcp | http | SF | 239 | 486  | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8 | 8 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 | 0.00 | 0.00 | 19 | 19 | 1.00 | 0.00 | 0.05 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | normal. | \n",
    "| 0 | tcp | http | SF | 235 | 1337 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8 | 8 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 | 0.00 | 0.00 | 29 | 29 | 1.00 | 0.00 | 0.03 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | normal. | \n",
    "| 0 | tcp | http | SF | 219 | 1337 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 6 | 6 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 | 0.00 | 0.00 | 39 | 39 | 1.00 | 0.00 | 0.03 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | normal. | \n",
    "\n",
    "\n",
    "\n",
    "# Getting data in Spark or RDD creation\n",
    "\n",
    "Apache Spark data structure:\n",
    "\n",
    "- **RDD (Resilient Distributed Datasets)**: Distributed collection of elements \n",
    "\n",
    "All work in Spark is expressed as:\n",
    "\n",
    "- **Creating** new RDDs\n",
    "- **Transforming** existing RDDs\n",
    "- **Calling actions** on RDDs to compute a result\n",
    "\n",
    "The most common way of creating an RDD is to load it from a file. \n",
    "\n",
    "**Spark's textFile** can handle compressed files directly.\n",
    "\n",
    "## Method a) Creating RDD from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.17.0.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initiallizing Spark (just run once)\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Retrieve data (don't forget to add the retrieve)\n",
    "import urllib.request\n",
    "f = urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")\n",
    "\n",
    "#Loading data\n",
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494021"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count the number of lines in the file \n",
    "raw_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,219,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,39,39,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,217,2032,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,49,49,1.00,0.00,0.02,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check first few entries\n",
    "raw_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method b) Creating RDD from a list using parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Existing list\n",
    "a = range(100)\n",
    "\n",
    "data = sc.parallelize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of elements on RDD\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the first elements\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic transformations and actions\n",
    "\n",
    "There are two basic classes of distributed unary (only one input) operations for RDD's, transformations and actions. \n",
    "\n",
    "- **The transformations** are distributed operations whose result generates a **_new RDD from another_**. they are **lazy**, because they are not performed until an action is called.\n",
    "- **The actions** are operations that apply a function of aggregation (such as mean, median,...) or others in a **_systematic and recurrent way to all the RDD_**. Return a value other than an RDD. Is performed immediatly.\n",
    "\n",
    "## Python's lambdas\n",
    "\n",
    "Python supports the creation of anonymous functions (i.e. functions that are not bound to a name) at runtime, using a construct called *lambda*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "g = lambda x: x**2\n",
    "print(g(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "Transformations are distributed operations whose result generates a new RDD from another\n",
    "\n",
    "### `filter` \n",
    "\n",
    "Applied to RDDs to keep only elements that satisfy certain conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example: counting normal interactions in the dataset\n",
    "## Consider you are not passing arrays but strings, so finding the pattern 'normal.' is straightforward.\n",
    "normal_raw_data = raw_data.filter(lambda x: 'normal.' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 97278 'normal' interactions\n",
      "Count completed in 0.827 seconds\n"
     ]
    }
   ],
   "source": [
    "# How many elements are in the new RDD?\n",
    "from time import time\n",
    "t0 = time()\n",
    "normal_count = normal_raw_data.count()\n",
    "tt = time() - t0\n",
    "print (\"There are {} 'normal' interactions\".format(normal_count))\n",
    "print (\"Count completed in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a total of 494021 in our 10 percent dataset, 97278 contain the *normal* tag word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `map`\n",
    "\n",
    "Map applies a function to every element in the RDD. In general Python's lambdas are useful for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse completed in 0.023 seconds\n"
     ]
    }
   ],
   "source": [
    "# To read a file as a .csv formated one:\n",
    "from pprint import pprint\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "#csv_data=raw_data.map(lambda x: str(x)[0:3])\n",
    "t0 = time()\n",
    "head_rows = csv_data.take(5)\n",
    "tt = time() - t0\n",
    "print (\"Parse completed in {} seconds\".format(round(tt,3)))\n",
    "\n",
    "#print(\"{}\".format(head_rows))\n",
    "\n",
    "#pprint(head_rows[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse completed in 1.165 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "head_rows = csv_data.take(100000)\n",
    "tt = time() - t0\n",
    "print (\"Parse completed in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `map` with predefined functions\n",
    "\n",
    "`map` can be used with predefined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Have each element as a key-value par where the key is the tag (\"normal.\" in this case in column 41)\n",
    "\n",
    "def parse_interaction(line):\n",
    "    elems = line.split(\",\")\n",
    "    tag = elems[41]\n",
    "    return (tag,elems)\n",
    "\n",
    "key_csv_data = raw_data.map(parse_interaction)\n",
    "head_rows = key_csv_data.take(5)\n",
    "#pprint(head_rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sample`\n",
    "\n",
    "Takes up to three parameters:\n",
    "\n",
    "1. Whether the sampling is done with replacement (True) or not (False). \n",
    "2. Second is the sample size as a fraction. \n",
    "3. Finally we can optionally provide a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size is 49493 of 494021\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "raw_data_sample = raw_data.sample(False, 0.1, 1234)\n",
    "sample_size = raw_data_sample.count()\n",
    "total_size = raw_data.count()\n",
    "print (\"Sample size is {} of {}\".format(sample_size,total_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of 'normal' interactions is 0.195\n",
      "Count done in 0.911 seconds\n"
     ]
    }
   ],
   "source": [
    "# Naive example: Approximation of the proportion of \"normal.\" with the .csv data\n",
    "##csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "raw_data_sample_items = raw_data_sample.map(lambda x: x.split(\",\"))\n",
    "sample_normal_tags = raw_data_sample_items.filter(lambda x: \"normal.\" in x)\n",
    "\n",
    "# actions + time\n",
    "t0 = time()\n",
    "sample_normal_tags_count = sample_normal_tags.count()\n",
    "tt = time() - t0\n",
    "\n",
    "sample_normal_ratio = sample_normal_tags_count / float(sample_size)\n",
    "print (\"The ratio of 'normal' interactions is {}\".format(round(sample_normal_ratio,3)) )\n",
    "print (\"Count done in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of 'normal' interactions is 0.197\n",
      "Count done in 1.657 seconds\n"
     ]
    }
   ],
   "source": [
    "#Continuation of naive example: Calculation without sampling (whole data)\n",
    "#Separating by , and filtering the normal ones\n",
    "raw_data_items = raw_data.map(lambda x: x.split(\",\"))\n",
    "normal_tags = raw_data_items.filter(lambda x: \"normal.\" in x)\n",
    "\n",
    "# actions + time\n",
    "t0 = time()\n",
    "normal_tags_count = normal_tags.count()\n",
    "tt = time() - t0\n",
    "\n",
    "normal_ratio = normal_tags_count / float(total_size)\n",
    "print (\"The ratio of 'normal' interactions is {}\".format(round(normal_ratio,3)))\n",
    "print (\"Count done in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more transformations and actions we apply after the sampling the bigger this gain vs using whole data. This is because without sampling all the transformations are applied to the complete set of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample vs **takeSample** action.\n",
    "\n",
    "If what we need is to grab a sample of raw data from our RDD into local memory in order to be used by other non-Spark libraries, `takeSample` can be used.\n",
    "\n",
    "### `ReducebyKey`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', 3), ('F', 3)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count or do something given a provided key in tuples\n",
    "people = sc.textFile(\"people.txt\")\n",
    "people=people.map(lambda x: x.split('\\t'))\n",
    "people.map(lambda t:(t[1],1)).reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "\n",
    "Are operations that apply a function of aggregation (such as mean, median,...) or others in a systematic and recurrent way to all the RDD.\n",
    "\n",
    "### `Collect`\n",
    "\n",
    "Gets all elements in the RDD into memory. Be careful with large RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected in 2.557 seconds\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "\n",
    "t0 = time()\n",
    "all_raw_data = raw_data.collect()\n",
    "tt = time() - t0\n",
    "print (\"Data collected in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every Spark worker node that has a fragment of the RDD has to be coordinated in order to retrieve its part, and then *reduce* everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected in 2.354 seconds\n",
      "There are 97278 'normal' interactions\n"
     ]
    }
   ],
   "source": [
    "# So to collect in memory all the normal interactions as key-value pairs:\n",
    "\n",
    "## get data from file\n",
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "\n",
    "# parse into key-value pairs with the previously created parse_interaction function in the map section\n",
    "##def parse_interaction(line):\n",
    "  ##  elems = line.split(\",\")\n",
    "  ##  tag = elems[41]\n",
    "  ##  return (tag,elems)\n",
    "\n",
    "key_csv_data = raw_data.map(parse_interaction)\n",
    "\n",
    "# collect all normal as defined in the previous filter\n",
    "## normal_raw_data = raw_data.filter(lambda x: 'normal.' in x)\n",
    "normal_key_interactions = key_csv_data.filter(lambda x: x[0] == \"normal.\")\n",
    "\n",
    "t0 = time()\n",
    "all_normal = normal_key_interactions.collect()\n",
    "tt = time() - t0\n",
    "normal_count = len(all_normal)\n",
    "print (\"Data collected in {} seconds\".format(round(tt,3)))\n",
    "print (\"There are {} 'normal' interactions\".format(normal_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.count` vs `collect + len`  \n",
    "\n",
    "- **`collect + len`** matches with the previous count for normal interactions. \n",
    "- **`collect + len`** is more time consuming because we retrieve all the data and then use Python's len on the resulting list\n",
    "- with **`count`** we were just counting the total number of elements in the RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### `Reduce`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `takeSample`\n",
    "\n",
    "Grab a sample of raw data from our RDD into local memory in order to be used by other non-Spark libraries.\n",
    "\n",
    "Characteristics:\n",
    "\n",
    "- Sintaxis very similar to sample, but you specify the number of items instead of the sample size:\n",
    "    1. Whether the sampling is done with replacement (True) or not (False). \n",
    "    2. Second is the number of items to be chosen. \n",
    "    3. Finally we can optionally provide a random seed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of 'normal' interactions is 0.1967175\n",
      "Assignation of sample done in 0.034 seconds with sample, vs 2.329 seconds with takeSample\n"
     ]
    }
   ],
   "source": [
    "# Take a sample for other libraries\n",
    "t0_1 = time()\n",
    "raw_data_sample = raw_data.sample(False, 0.1, 1234)\n",
    "#raw_data_sample_items = raw_data_sample.map(lambda x: x.split(\",\"))\n",
    "#sample_normal_tags = raw_data_sample_items.filter(lambda x: \"normal.\" in x)\n",
    "tt_1 = time() - t0_1\n",
    "\n",
    "\n",
    "t0_2 = time()\n",
    "raw_data_sample = raw_data.takeSample(False, 400000, 1234)\n",
    "normal_data_sample = [x.split(\",\") for x in raw_data_sample if \"normal.\" in x]\n",
    "tt_2 = time() - t0_2\n",
    "\n",
    "normal_sample_size = len(normal_data_sample)\n",
    "\n",
    "normal_ratio = normal_sample_size / 400000.0\n",
    "print (\"The ratio of 'normal' interactions is {}\".format(normal_ratio))\n",
    "#print (\"Assignation, splitting and filtering done in {} seconds with sample, vs {} seconds with takeSample\".format(round(tt_1,3),round(tt_2,3)))\n",
    "print (\"Assignation of sample done in {} seconds with sample, vs {} seconds with takeSample\".format(round(tt_1,3),round(tt_2,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took longer, even with a slightly smaller sample. The reason is that Spark just distributed the execution of the sampling process. The filtering and splitting of the results were done locally in a single node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `first`\n",
    "\n",
    "Gives you the first element of an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(1,10)).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercises from https://www.youtube.com/watch?v=9xYfNznjClE\n",
    "\n",
    "Given a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [(1,2),(2,4),(5,6)]\n",
    "data = sc.parallelize(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write an expression to get only the second elements of each tupple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "#Remember that map is a methos of a RDD\n",
    "data2 = data.map(lambda x:x[1])\n",
    "print(data2.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write an expression to get the sum of the second elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.reduce(lambda y,z:y + z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write an expression to get the sum of the odd first elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "data1 = data.map(lambda x:x[0])\n",
    "data1_1=data1.map(lambda y: y%2==1)\n",
    "#If you use map instead of filter it won't do what the instruction indicated, \n",
    "# because, even if it will find which one  \n",
    "print(data1_1.count())\n",
    "print(data1_1.reduce(lambda b, c: b +c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same as the previous but with filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "data3 = data.map(lambda x:x[0])\n",
    "#data1.map(lambda y: y%2==1)\n",
    "\n",
    "data4 = data3.filter(lambda y: y%2==1)\n",
    "print(data4.count())\n",
    "print(data4.reduce(lambda b, c: b +c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(1,10)).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lazy took 0.0025794506072998047 seconds vs active which took 0.017164945602416992 seconds. Which is 6.654496718735558 times fold\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "list2=sc.parallelize( range(1,1000) ).map(lambda x: x*10)\n",
    "tt = time() - t0\n",
    "#it does nothing, that is why it seems so fats, it is lazzzzzy u.u \n",
    "t0 = time()\n",
    "list2.first()\n",
    "tt2 = time() - t0\n",
    "print(\"Lazy took {} seconds vs active which took {} seconds. Which is {} times fold\".format(tt,tt2,tt2/tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce took 0.03945279121398926 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "list2.reduce(lambda x,y : x + y )\n",
    "tt = time() - t0 \n",
    "print(\"Reduce took {} seconds\".format(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 200, 300]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list2.filter(lambda x : x%100==0).take(3)\n",
    "\n",
    "#list2.filter(lambda x : x%100==0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3316.6666666666665"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1=sc.parallelize( range(1,100) )\n",
    "#rdd1.map(lambda x: x*x).sum()\n",
    "#rdd1.map(lambda x: x*x).max()\n",
    "#rdd1.map(lambda x: x*x).min()\n",
    "rdd1.map(lambda x: x*x).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "1. Get an RDD with numbers 1 to 10\n",
    "2. Get all the elements in that RDD which are divisible by 3\n",
    "3. Get the product of the elements in 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3=sc.parallelize( range(1,10) )\n",
    "#rdd3.filter(lambda x: x%3 == 0).take(3)\n",
    "rdd3.filter(lambda x: x%3 == 0).reduce(lambda x,y: x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize( range(1,10) ).filter(lambda x: x%3 == 0).reduce(lambda x,y: x*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More excercises (about reading files)\n",
    "The data comes from https://github.com/okaram/spark-pycon15/blob/master/data/people.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Orlando\\tM\\t40\\tPython', 'Lina\\tF\\t39\\tC#']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = sc.textFile(\"people.txt\")\n",
    "people.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Orlando', 'M', '40', 'Python']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split lines in people by tabs and take the first line\n",
    "people.map(lambda x: x.split('\\t')).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Orlando', 'M', '40', 'Python'],\n",
       " ['Lina', 'F', '39', 'C#'],\n",
       " ['John', 'M', '30', 'Python']]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people=people.map(lambda x: x.split('\\t'))\n",
    "people.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', 1)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define keys by making dummy tuples\n",
    "#people.map(lambda t:(t[1],1)).take(1)\n",
    "people.map(lambda t:(t[1],1)).take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', 3), ('F', 3)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#people.map(lambda t:(t[1],1)).take(1)\n",
    "people.map(lambda t:(t[1],1)).reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', 40), ('F', 39), ('M', 30), ('F', 32), ('F', 18), ('M', 20)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Youngest per gender\n",
    "#we use int to avoid interpretation as string\n",
    "people.map(lambda t:(t[1],int(t[2]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', 20), ('F', 18)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.map(lambda t:(t[1],int(t[2]))).reduceByKey(lambda x,y: min(x,y)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', 40), ('F', 39)]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.map(lambda t:(t[1],int(t[2]))).reduceByKey(lambda x,y: max(x,y)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Person( Orlando, gender=M, 40 years old, likes Python),\n",
       " Person( Lina, gender=F, 39 years old, likes C#),\n",
       " Person( John, gender=M, 30 years old, likes Python)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing my python code\n",
    "import person\n",
    "people = sc.textFile(\"people.txt\").map(lambda l: person.Person().parse(l))\n",
    "people.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', 20), ('F', 18)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Once having done this, we can use much more informative tags\n",
    "people.map(lambda t:(t.gender,t.age)).reduceByKey(lambda x,y: min(x,y)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', 3), ('F', 3)]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.map(lambda t:(t.gender,1)).reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2014-01-01', '1', '1', '100'],\n",
       " ['2014-01-01', '1', '2', '37'],\n",
       " ['2014-01-01', '1', '3', '54'],\n",
       " ['2014-01-01', '2', '1', '50'],\n",
       " ['2014-01-01', '2', '2', '40'],\n",
       " ['2014-01-01', '3', '1', '75'],\n",
       " ['2014-01-01', '4', '4', '1'],\n",
       " ['2014-01-02', '1', '1', '10'],\n",
       " ['2014-01-02', '1', '2', '31'],\n",
       " ['2014-01-02', '1', '3', '5'],\n",
       " ['2014-01-02', '2', '1', '52'],\n",
       " ['2014-01-02', '2', '2', '42'],\n",
       " ['2014-01-02', '3', '2', '72']]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with * hadoop can do the file replacement\n",
    "sales=sc.textFile(\"sales_*.txt\").map(lambda x:x.split('\\t'))\n",
    "sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 37, 54, 50, 40, 75, 1, 10, 31, 5, 52, 42, 72]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.map(lambda x:int(x[3])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.map(lambda x:int(x[3])).reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AK', ('Alaska', 710231)),\n",
       " ('AR', ('Arizona', 6392017)),\n",
       " ('AL', ('Alabama', 4779736))]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states=[(\"AL\",\"Alabama\"),\n",
    "       (\"AK\",\"Alaska\"),\n",
    "       (\"AR\",\"Arizona\")\n",
    "       ];\n",
    "populations=[(\"AL\",4779736),\n",
    "       (\"AK\",710231),\n",
    "       (\"AR\",6392017)\n",
    "       ];\n",
    "\n",
    "states_rdd=sc.parallelize(states)\n",
    "populations_rdd=sc.parallelize(populations)\n",
    "states_rdd.join(populations_rdd).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Table or Data Frame\n",
    "\n",
    "- Knows its fields, has no methods\n",
    "- Similar to a data table in pandas\n",
    "- All rows must have the same fields\n",
    "- Queries can be done such as in RDDs or through SQL\n",
    "\n",
    "\n",
    "## Reading a .json as datatable (and its methods)\n",
    "\n",
    "\n",
    "In this case, several methods can be used:\n",
    "\n",
    "- .select: like map, can use strings or columns\n",
    "```python\n",
    "people.select(\"name\"people.age+1).show()\n",
    "```\n",
    "- .filter: can filter certain rows\n",
    "```python\n",
    "people.filter(people.age>30).show()\n",
    "```\n",
    "- .where: similar to filter\n",
    "\n",
    "- .show: displays nicely\n",
    "- Use pandas syntax for filter\n",
    "```python\n",
    "people[people.gender=='F']\n",
    "```\n",
    "- GroupBy: returns a filtered RDD\n",
    "```python\n",
    "people.groupBy(people.gender]).count()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>favorite_language</th>\n",
       "      <th>gender</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>Python</td>\n",
       "      <td>M</td>\n",
       "      <td>Orlando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>C#</td>\n",
       "      <td>F</td>\n",
       "      <td>Lina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>Python</td>\n",
       "      <td>M</td>\n",
       "      <td>John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>Python</td>\n",
       "      <td>F</td>\n",
       "      <td>Jane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>Python</td>\n",
       "      <td>F</td>\n",
       "      <td>Michelle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>C#</td>\n",
       "      <td>M</td>\n",
       "      <td>Daniel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age favorite_language gender      name\n",
       "0   40            Python      M   Orlando\n",
       "1   39                C#      F      Lina\n",
       "2   30            Python      M      John\n",
       "3   32            Python      F      Jane\n",
       "4   18            Python      F  Michelle\n",
       "5   20                C#      M    Daniel"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Needs to use sqlCtx.jsonFile sql Context\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "people=sqlContext.read.json(\"people.json\")\n",
    "\n",
    "#Terminal friendly\n",
    "#people.show()\n",
    "\n",
    "#User friendly aspect in jupyter (also terminal friendly)\n",
    "people.limit(10).toPandas()\n",
    "\n",
    "#people.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=40, favorite_language='Python', gender='M', name='Orlando')"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .select (similar to lambda but not tht cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    name|\n",
      "+--------+\n",
      "| Orlando|\n",
      "|    Lina|\n",
      "|    John|\n",
      "|    Jane|\n",
      "|Michelle|\n",
      "|  Daniel|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|    name|(age + 1)|\n",
      "+--------+---------+\n",
      "| Orlando|       41|\n",
      "|    Lina|       40|\n",
      "|    John|       31|\n",
      "|    Jane|       33|\n",
      "|Michelle|       19|\n",
      "|  Daniel|       21|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(\"name\",people.age+1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### .filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+------+-------+\n",
      "|age|favorite_language|gender|   name|\n",
      "+---+-----------------+------+-------+\n",
      "| 40|           Python|     M|Orlando|\n",
      "| 39|               C#|     F|   Lina|\n",
      "| 32|           Python|     F|   Jane|\n",
      "+---+-----------------+------+-------+\n",
      "\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 35.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "people.filter(people.age>30).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+------+-------+\n",
      "|age|favorite_language|gender|   name|\n",
      "+---+-----------------+------+-------+\n",
      "| 40|           Python|     M|Orlando|\n",
      "| 39|               C#|     F|   Lina|\n",
      "| 32|           Python|     F|   Jane|\n",
      "+---+-----------------+------+-------+\n",
      "\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 40.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "people.where(people.age>30).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pandas syntax for filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+------+--------+\n",
      "|age|favorite_language|gender|    name|\n",
      "+---+-----------------+------+--------+\n",
      "| 39|               C#|     F|    Lina|\n",
      "| 32|           Python|     F|    Jane|\n",
      "| 18|           Python|     F|Michelle|\n",
      "+---+-----------------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people[people.gender=='F'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two pandas filter in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+------+----+\n",
      "|age|favorite_language|gender|name|\n",
      "+---+-----------------+------+----+\n",
      "| 39|               C#|     F|Lina|\n",
      "| 32|           Python|     F|Jane|\n",
      "+---+-----------------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people[people.age>30][people.gender==\"F\"].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+------+----+\n",
      "|age|favorite_language|gender|name|\n",
      "+---+-----------------+------+----+\n",
      "| 39|               C#|     F|Lina|\n",
      "| 32|           Python|     F|Jane|\n",
      "+---+-----------------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people[(people.age>30) & (people.gender==\"F\")].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy  (returns a grouped RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|     F|    3|\n",
      "|     M|    3|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.groupBy(people.gender).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|gender|          avg(age)|\n",
      "+------+------------------+\n",
      "|     F|29.666666666666668|\n",
      "|     M|              30.0|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.groupBy(people.gender).mean('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use table as SQL\n",
    "\n",
    "1. Register as TempTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people.registerTempTable(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "| Orlando| 40|\n",
      "|    Lina| 39|\n",
      "|    John| 30|\n",
      "|    Jane| 32|\n",
      "|Michelle| 18|\n",
      "|  Daniel| 20|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select name, age FROM people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|gender|            AvgAge|\n",
      "+------+------------------+\n",
      "|     F|29.666666666666668|\n",
      "|     M|              30.0|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select gender, avg(age) AS AvgAge FROM people GROUP BY gender\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance\n",
    "\n",
    "- Spark in python is slower than scala due to translation\n",
    "- Datatable (dataFrame) avoids this translation (all lives in JVm until the last step to client)\n",
    "- Datatable should be able to optimize better, but you lose control\n",
    "- Join and reduce are very expensive\n",
    "\n",
    "Spark keeps everything in RAM, if you need the RAM for something else, spark kills the part used and recalculates each time :).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources:\n",
    "    \n",
    "- [Markdown Cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)\n",
    "- [Lambda functions in python](http://www.secnetix.de/olli/Python/lambda_functions.hawk)\n",
    "- [Introduction to Spark with python](https://www.youtube.com/watch?v=9xYfNznjClE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
